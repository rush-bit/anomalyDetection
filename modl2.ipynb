{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bdd22d",
   "metadata": {},
   "source": [
    "### This is the main Model which we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3895f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "import shap\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e7736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EBM model for cluster 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saxen\\anaconda3\\Lib\\site-packages\\interpret\\glassbox\\_ebm\\_ebm.py:869: UserWarning: Missing values detected. Our visualizations do not currently display missing values. To retain the glassbox nature of the model you need to either set the missing values to an extreme value like -1000 that will be visible on the graphs, or manually examine the missing value score in ebm.term_scores_[term_index][0]\n",
      "  warn(\n",
      "PermutationExplainer explainer: 4170it [03:22, 20.16it/s]                          \n",
      "C:\\Users\\saxen\\AppData\\Local\\Temp\\ipykernel_23936\\2596839304.py:64: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
      "  shap.summary_plot(shap_values, X, show=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EBM model for cluster 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saxen\\anaconda3\\Lib\\site-packages\\interpret\\glassbox\\_ebm\\_ebm.py:869: UserWarning: Missing values detected. Our visualizations do not currently display missing values. To retain the glassbox nature of the model you need to either set the missing values to an extreme value like -1000 that will be visible on the graphs, or manually examine the missing value score in ebm.term_scores_[term_index][0]\n",
      "  warn(\n",
      "PermutationExplainer explainer: 622it [00:14, 13.88it/s]                         \n",
      "C:\\Users\\saxen\\AppData\\Local\\Temp\\ipykernel_23936\\2596839304.py:64: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
      "  shap.summary_plot(shap_values, X, show=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Isolation Forest (unsupervised)...\n",
      "\n",
      "Completed EBM training, Isolation Forest, scoring, explanation, and alerting.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Model Creation Function\n",
    "def create_ebm_model(feature_cols, monotone_features=None):\n",
    "    \"\"\"\n",
    "    Create Explainable Boosting Machine regressor with monotonicity constraints.\n",
    "    :param feature_cols: list of feature column names\n",
    "    :param monotone_features: dict {feature_name: +1/-1/0}, e.g. {'speed_mean': 1}\n",
    "    :return: ebm model instance\n",
    "    \"\"\"\n",
    "    monotonic_terms = {}\n",
    "    if monotone_features is not None:\n",
    "        for feat, mono in monotone_features.items():\n",
    "            if feat in feature_cols:\n",
    "                monotonic_terms[feature_cols.index(feat)] = mono\n",
    "\n",
    "    ebm = ExplainableBoostingRegressor()\n",
    "    return ebm\n",
    "\n",
    "\n",
    "# 2. Data Loading Function\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "# 3. Training Function Per Cluster (EBM residual-based anomalies)\n",
    "def train_cluster_models(df, feature_cols, target_col, monotone_features=None):\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    os.makedirs('explanations', exist_ok=True)\n",
    "\n",
    "    df['anomaly_score_ebm'] = np.nan\n",
    "    df['alert_ebm'] = False\n",
    "\n",
    "    for cluster_id in df['cluster'].unique():\n",
    "        df_cluster = df[df['cluster'] == cluster_id].copy()\n",
    "\n",
    "        # Skip very small clusters\n",
    "        if len(df_cluster) < 20:\n",
    "            print(f\"Skipping cluster {cluster_id} (too few samples)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Training EBM model for cluster {cluster_id}\")\n",
    "\n",
    "        X = df_cluster[feature_cols]\n",
    "        y = df_cluster[target_col]\n",
    "\n",
    "        model = create_ebm_model(feature_cols, monotone_features)\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Save model\n",
    "        model_path = f'models/ebm_regressor_cluster_{cluster_id}.joblib'\n",
    "        joblib.dump(model, model_path)\n",
    "\n",
    "        # Predict and residuals (standardized)\n",
    "        y_pred = model.predict(X)\n",
    "        residuals = np.abs(y - y_pred)\n",
    "        residuals = (residuals - residuals.mean()) / residuals.std()\n",
    "        df.loc[df['cluster'] == cluster_id, 'anomaly_score_ebm'] = residuals\n",
    "\n",
    "        # SHAP explainability\n",
    "        explainer = shap.Explainer(model.predict, X)\n",
    "        shap_values = explainer(X)\n",
    "\n",
    "        # Save summary plot\n",
    "        shap.summary_plot(shap_values, X, show=False)\n",
    "        plt.savefig(f'explanations/shap_summary_cluster_{cluster_id}.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Save force plot of first row\n",
    "        force_plot = shap.force_plot(\n",
    "            shap_values.base_values[0],\n",
    "            shap_values.values[0],\n",
    "            X.iloc[0],\n",
    "            matplotlib=False\n",
    "        )\n",
    "        with open(f'explanations/shap_force_cluster_{cluster_id}.html', \"w\") as f:\n",
    "            f.write(shap.getjs() + force_plot.html())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# 4. Adaptive Threshold & Alerting for EBM\n",
    "def generate_alerts_ebm(df):\n",
    "    for cluster_id in df['cluster'].unique():\n",
    "        cluster_scores = df.loc[df['cluster'] == cluster_id, 'anomaly_score_ebm'].dropna()\n",
    "        if len(cluster_scores) < 10:\n",
    "            continue\n",
    "        threshold = np.percentile(cluster_scores, 95)\n",
    "        df.loc[(df['cluster'] == cluster_id) &\n",
    "               (df['anomaly_score_ebm'] > threshold), 'alert_ebm'] = True\n",
    "    return df\n",
    "\n",
    "\n",
    "# 5. Isolation Forest Unsupervised Detector\n",
    "def run_isolation_forest(df, feature_cols):\n",
    "    print(\"\\nRunning Isolation Forest (unsupervised)...\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "    iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "    preds = iso.fit_predict(X_scaled)  # -1 = anomaly, 1 = normal\n",
    "    scores = -iso.decision_function(X_scaled)  # higher = more anomalous\n",
    "\n",
    "    df['anomaly_score_iforest'] = scores\n",
    "    df['alert_iforest'] = preds == -1\n",
    "    return df\n",
    "\n",
    "\n",
    "# 6. Main Execution Flow\n",
    "if __name__ == '__main__':\n",
    "    df = load_data('CSV_files/final_clustered_data.csv')\n",
    "\n",
    "    # Specify features and target (adjust as needed)\n",
    "    feature_cols = [col for col in df.columns if col not in\n",
    "                    ['deviceID', 'tripID', 'cluster', 'label',\n",
    "                     'anomaly_score_ebm', 'alert_ebm',\n",
    "                     'anomaly_score_iforest', 'alert_iforest']]\n",
    "    target_col = 'kpl_mean'  # example target â€” change if not in dataset\n",
    "\n",
    "    # Specify monotonic features (domain knowledge example)\n",
    "    monotone_features = {'speed_mean': 1, 'rpm_mean': 1}\n",
    "\n",
    "    # 1. Train EBM residual-based models\n",
    "    df = train_cluster_models(df, feature_cols, target_col, monotone_features)\n",
    "\n",
    "    # 2. Generate alerts (EBM)\n",
    "    df = generate_alerts_ebm(df)\n",
    "\n",
    "    # 3. Run Isolation Forest (unsupervised)\n",
    "    df = run_isolation_forest(df, feature_cols)\n",
    "\n",
    "    # Save final data with anomaly scores and alerts\n",
    "    df.to_csv('CSV_files/Final_Anomalies_Clustered_data.csv', index=False)\n",
    "    print(\"\\nCompleted EBM training, Isolation Forest, scoring, explanation, and alerting.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
